{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "3nf2krt5xxodrfffofez",
   "authorId": "5147980021417",
   "authorName": "ZINEB",
   "authorEmail": "aliouiiiy@gmail.com",
   "sessionId": "280edb65-b7fc-4eb3-91ad-ce10cf9b34fe",
   "lastEditTime": 1767938086507
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5cd72fb-e39c-4450-9983-ec27d9ecc658",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "# Anonymization Pipeline\n - Suppression (masking)\n - Perturbation (généralisation / bruit)\n - Tokenisation (pseudonymisation stable)"
  },
  {
   "cell_type": "markdown",
   "id": "2599a545-69cc-42ea-91bf-fff18033f0f8",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "    Classe qui encapsule plusieurs stratégies d'anonymisation sémantique\n    pour des entités PII détectées dans du texte.\n    \n    Techniques couvertes :\n    - Suppression : remplacement par un token générique\n    - Perturbation : âge en tranche, montant arrondi\n    - Tokenisation : pseudonymes stables par entité (PERSONNE_1, ORG_2, ...)"
  },
  {
   "cell_type": "code",
   "id": "95a4ecd1-8c7f-4a7b-ae3f-c75f0a78ce7e",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "import snowflake.snowpark as snp\nfrom snowflake.snowpark.context import get_active_session\nimport pandas as pd\nfrom collections import defaultdict\nimport re\n\n# Récupère la session Snowpark active dans le notebook Snowflake\nsession = get_active_session()\n\nclass SemanticAnonymizer:\n    \n    def __init__(self):\n        # mapping : (entity_type, original_value) -> pseudonym\n        # permet de garantir que la même valeur reçoit toujours le même pseudonyme\n        self.mapping = {}\n        \n        # counters : compteur par type d'entité (PER, ORG, LOC, ...)\n        # sert à générer PERSONNE_1, PERSONNE_2, ORG_1, etc.\n        self.counters = defaultdict(int)\n    \n    # 1. Suppression (masking)\n    def suppress(self, value, entity_type):\n        \"\"\"\n        Suppression totale : remplace la valeur par un token générique\n        ex : \"Jean Dupont\" (PER) -> [PER_SUPPRIMÉ]\n        \"\"\"\n        return f\"[{entity_type.upper()}_SUPPRIMÉ]\"\n        \n    # 2. Perturbation d'âge (généralisation / bucketing)\n    def perturb_age(self, age_str):\n        \"\"\"\n        Perturbation d'âge : remplace un âge précis par une tranche d'âges.\n        Technique = généralisation / bucketing pour réduire la précision\n        \"\"\"\n        match = re.search(r'(\\d+)', age_str)\n        if match:\n            age = int(match.group(1))\n            start = (age // 10) * 10\n            end = start + 10\n            return f\"[{start}-{end} ans]\"\n        # Si aucun nombre trouvé, on renvoie la chaîne originale\n        return age_str\n    \n    # 3. Perturbation de montant (arrondi / bruit contrôlé)\n    def perturb_amount(self, amount_str):\n        \"\"\"\n        Perturbation de montant : arrondit le montant à ~1000€ près.\n        Technique = perturbation / généralisation pour garder l'ordre de grandeur.\n        \"\"\"\n        match = re.search(r'(\\d+)', amount_str)\n        if match:\n            amount = int(match.group(1))\n            rounded = round(amount / 1000) * 1000\n            # Utilisation de séparateur de milliers pour lisibilité\n            return f\"~{rounded:,}€\"\n        return amount_str\n    \n    # 4. Tokenisation / pseudonymisation stable\n    def tokenize(self, value, entity_type):\n        \"\"\"\n     Tokenisation / pseudonymisation : remplace la valeur par un pseudonyme lisible\n        - garantit que la même valeur reçoit toujours le même pseudonyme,\n          indépendamment du nombre d'occurrences.\n        \"\"\"\n        \n        # Clé = couple (type, valeur) pour distinguer mêmes chaînes de types différents\n        key = (entity_type, value)\n        \n        # Si on n'a jamais vu cette valeur pour ce type, on crée un nouveau pseudo\n        if key not in self.mapping:\n            # Incrémente le compteur pour ce type d'entité\n            self.counters[entity_type] += 1\n            count = self.counters[entity_type]\n            \n            # Construit un pseudonyme lisible selon le type\n            if entity_type == 'PER':\n                pseudo = f'PERSONNE_{count}'\n            elif entity_type == 'ORG':\n                pseudo = f'ORG_{count}'\n            elif entity_type == 'LOC':\n                pseudo = f'LOC_{count}'\n            else:\n                #  générique pour les autres types (SIRET, EMAIL, IBAN, etc.)\n                pseudo = f'{entity_type}_{count}'\n            \n            # Enregistre le mapping pour pouvoir réutiliser le même pseudo\n            self.mapping[key] = pseudo\n        \n        # Retourne toujours le même pseudonyme pour ce (type, valeur)\n        return self.mapping[key]\n\n\n\n\n    \n    # 5. Export du mapping sous forme de table\n    def get_mapping_table(self):\n        return [\n            {'pseudonym': pseudo, 'original_value': orig, 'entity_type': etype}\n            for (etype, orig), pseudo in self.mapping.items()\n        ]\n\n\nanon_global = SemanticAnonymizer()\nprint(\"Classe SemanticAnonymizer créée et prête à l'emploi\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc12f496-625e-47ee-88cb-7bdc6d88fad8",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Retourne le mapping sous forme de liste de dictionnaires,\nexploitable directement comme DataFrame / table Snowflake :\n        \n        [\n          {'pseudonym': 'PERSONNE_1', 'original_value': 'Jean Brunel', 'entity_type': 'PER'},\n          {'pseudonym': 'ORG_1',      'original_value': 'ACME SAS',   'entity_type': 'ORG'},\n          ...\n        ]\n        \nCela permet de stocker ce mapping dans PROCESSED_DATA.PII_MAPPING pour audit et éventuelle désanonymisation contrôlée.\n"
  },
  {
   "cell_type": "markdown",
   "id": "ac30d838-2279-4963-84d2-6b3e566ce315",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "    Applique une stratégie d'anonymisation pour chaque entité PII détectée\n    dans le texte, en s'appuyant sur l'instance SemanticAnonymizer.\n    \n    Paramètres\n    text : str\n        Texte original contenant des PII.\n    pii_list : list[dict]\n        Liste d'entités détectées, chaque élément doit au minimum contenir :\n        - 'type' : type d'entité (PER, ORG, LOC, AGE, MONTANT, SIRET, etc.)\n        - 'text' : valeur exacte trouvée dans le texte.\n    anon : SemanticAnonymizer\n        Instance de la classe gérant les stratégies d'anonymisation.\n\n    Retourne\n    anon_text : str\n        Texte anonymisé.\n    replacements : list[dict]\n        Liste des remplacements effectués (original, type, replacement).\n    mapping_table : list[dict]\n        Mapping global pseudonymes ↔ valeurs originales.\n"
  },
  {
   "cell_type": "code",
   "id": "7f97f20c-6c6a-4b74-9f75-4ffdd1ad7d8e",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "#  appliquer les stratégies d'anonymisation\n# sur une liste d'entités PII détectées dans un texte.\n\ndef anonymize_with_strategy(text, pii_list, anon: SemanticAnonymizer):\n    # Texte à anonymiser progressivement\n    anon_text = text\n    # Journal des remplacements\n    replacements = []\n\n    # 1. Trier les entités par position décroissante dans le texte\n    #    pour éviter que des remplacements courts cassent des entités plus longues.\n    sorted_pii = sorted(\n        pii_list,\n        key=lambda x: text.find(x[\"text\"]),\n        reverse=True\n    )\n\n    # 2. Première passe : IBAN en priorité \n    for pii in sorted_pii:\n        entity_type = pii[\"type\"]\n        original = pii[\"text\"]\n\n        if entity_type != \"IBAN\":\n            continue\n\n        # Stratégie dédiée : suppression ou masquage total de l'IBAN\n        replacement = anon.suppress(original, entity_type)\n\n        if original in anon_text:\n            anon_text = anon_text.replace(original, replacement)\n\n        replacements.append({\n            \"original\": original,\n            \"type\": entity_type,\n            \"replacement\": replacement\n        })\n\n    # 3. Deuxième passe : toutes les autres entités (dont TEL)\n    for pii in sorted_pii:\n        entity_type = pii[\"type\"]\n        original = pii[\"text\"]\n\n        # IBAN déjà traité à l'étape 2\n        if entity_type == \"IBAN\":\n            continue\n\n        # Stratégie par type d'entité\n        if entity_type in [\"PER\", \"ORG\", \"LOC\"]:\n            # Pseudonymisation des acteurs (personnes, organisations, lieux)\n            replacement = anon.tokenize(original, entity_type)\n\n        elif entity_type == \"AGE\":\n            # Généralisation de l'âge en tranche (40-50 ans, etc.)\n            replacement = anon.perturb_age(original)\n\n        elif entity_type == \"MONTANT\":\n            # Arrondi / perturbation des montants\n            replacement = anon.perturb_amount(original)\n\n        elif entity_type in [\n            \"SIRET\", \"SIREN\", \"ADRESSE\", \"CODE_POSTAL\",\n            \"DATE\", \"DATE_NAISSANCE\", \"EMAIL\", \"TEL\", \"NIR\"\n        ]:\n            # Identifiants / coordonnées très sensibles : suppression ou masquage total\n            replacement = anon.suppress(original, entity_type)\n\n        else:\n            # Par défaut : pseudonymisation générique\n            replacement = anon.tokenize(original, entity_type)\n\n        # Remplacer dans le texte anonymisé\n        if original in anon_text:\n            anon_text = anon_text.replace(original, replacement)\n\n        # Journaliser ce remplacement (utile pour audit / debug)\n        replacements.append({\n            \"original\": original,\n            \"type\": entity_type,\n            \"replacement\": replacement\n        })\n\n    # 4. Retourner le texte anonymisé, la liste des remplacements, et la table de mapping globale\n    return anon_text, replacements, anon.get_mapping_table()\n\n\nprint(\"Fonction d'anonymisation créée\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b038fa1a-36d0-4533-8ad5-9384834ae4da",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "# Boucle principale : appliquer l'anonymisation à tous les docs\n# et charger le résultat dans PROCESSED_DATA.FISCAL_DOCUMENTS_ANON\n\nfrom datetime import datetime\nimport pandas as pd\n\nprint(\"Lancement de l'anonymisation...\")\n\n# 1. Charger les documents bruts + PII détectées (CamemBERT+regex)\ndocs_with_pii = session.sql(\"\"\"\nSELECT \n    r.doc_id,\n    r.document_text,\n    p.pii_detected\nFROM raw_data.fiscal_documents_raw r\nLEFT JOIN processed_data.pii_detection_temp p \n    ON r.doc_id = p.doc_id\nORDER BY r.doc_id\n\"\"\").collect()\n\nall_results = []\n\n# 2. Boucle sur chaque document : construire la liste PII,\n#    appliquer la stratégie d'anonymisation.\n\nfor i, doc in enumerate(docs_with_pii):\n    doc_id = doc['DOC_ID']\n    original_text = doc['DOCUMENT_TEXT']\n    pii_variant = doc['PII_DETECTED']  # colonne VARIANT -> objet Python (liste/dict) dans Snowpark\n    \n    # 2.1 Normaliser la liste des PII\n    #     On veut une liste de dictionnaire avec au minimum la clé 'text'.\n    if pii_variant is not None:\n        # Garantir qu'on a bien une liste\n        raw_list = list(pii_variant) if not isinstance(pii_variant, list) else pii_variant\n        # Ne garder que les entrées exploitables par anonymize_with_strategy\n        pii_list = [\n            x for x in raw_list\n            if isinstance(x, dict) and 'text' in x\n        ]\n    else:\n        pii_list = []\n\n    # 2.2 Appliquer la fonction d'anonymisation\n    anon_text, replacements, mappings = anonymize_with_strategy(\n        original_text,\n        pii_list,\n        anon_global\n    )\n\n    # 2.3 Construire l'enregistrement de sortie pour ce document\n\n    all_results.append({\n        'DOC_ID': doc_id,\n        'ORIGINAL_TEXT': original_text,\n        'ANONYMIZED_TEXT': anon_text,\n        'PII_DETECTED': pii_list,          \n        'PII_COUNT': len(pii_list),\n        'STRATEGY_USED': replacements,     \n        'PROCESSED_AT': datetime.utcnow()  \n    })\n\n    # 2.4 Progression\n    if (i + 1) % 10 == 0:\n        print(f\"  ✓ {i+1}/{len(docs_with_pii)} documents anonymisés\")\n\nprint(f\"\\n Anonymisation complétée\")\n\n# 3. Charger les résultats dans PROCESSED_DATA.FISCAL_DOCUMENTS_ANON\n\ndf_anon = pd.DataFrame(all_results)\nsdf_anon = session.create_dataframe(df_anon)\n\n# mode(\"append\") : on ajoute les résultats à la table existante\nsdf_anon.write.mode(\"append\").save_as_table(\"processed_data.fiscal_documents_anon\")\n\nprint(f\" {len(all_results)} documents chargés dans processed_data.fiscal_documents_anon\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "876e02e5-8f6a-489d-a2a7-a306241790cf",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "doc_id_test = 1000  # adapte si besoin\n\nrow = (\n    session.sql(f\"\"\"\n        SELECT \n            r.DOC_ID,\n            r.DOCUMENT_TEXT,\n            p.PII_DETECTED\n        FROM RAW_DATA.FISCAL_DOCUMENTS_RAW r\n        LEFT JOIN PROCESSED_DATA.PII_DETECTION_TEMP p\n            ON r.DOC_ID = p.DOC_ID\n        WHERE r.DOC_ID = {doc_id_test}\n    \"\"\")\n    .collect()[0]\n)\n\noriginal_text = row[\"DOCUMENT_TEXT\"]\npii_variant = row[\"PII_DETECTED\"]\n\nprint(\"DOC_ID:\", row[\"DOC_ID\"])\nprint(\"\\n--- TEXTE ORIGINAL (début) ---\\n\")\nprint(original_text[:500])\nprint(\"\\n--- PII_DETECTED ---\\n\")\nprint(pii_variant)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45ffa801-6cdf-4e61-bf50-0cb7fc0b3d4b",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "SELECT DOC_ID, PII_COUNT, PII_DETECTED\nFROM PROCESSED_DATA.PII_DETECTION_TEMP\nORDER BY DOC_ID\nLIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7af19b7-ce09-4c9c-a14d-67dffeceda7f",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "import pandas as pd\n\ndf_join = (\n    session.sql(\"\"\"\n        SELECT \n            r.DOC_ID,\n            r.DOCUMENT_TEXT,\n            a.ANONYMIZED_TEXT,\n            a.PII_COUNT\n        FROM RAW_DATA.FISCAL_DOCUMENTS_RAW r\n        JOIN PROCESSED_DATA.FISCAL_DOCUMENTS_ANON a\n            ON r.DOC_ID = a.DOC_ID\n        ORDER BY r.DOC_ID\n        LIMIT 3\n    \"\"\")\n    .to_pandas()\n)\n\nfor idx in range(len(df_join)):\n    doc_id = df_join.loc[idx, \"DOC_ID\"]\n    original_text = df_join.loc[idx, \"DOCUMENT_TEXT\"]\n    anon_text = df_join.loc[idx, \"ANONYMIZED_TEXT\"]\n    pii_count = df_join.loc[idx, \"PII_COUNT\"]\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"DOC_ID : {doc_id}  |  PII détectées : {pii_count}\")\n    print(\"=\" * 80 + \"\\n\")\n    \n    print(\" AVANT (texte brut) :\\n\")\n    print(original_text)\n    \n    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n    \n    print(\" APRÈS (texte anonymisé) :\\n\")\n    print(anon_text)\n    \n    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c9e53f6-b25e-4c78-ada3-f9833dcd2146",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "from datetime import datetime\nimport pandas as pd\nimport json\n\nprint(\"Lancement de l'anonymisation...\")\n\ndocs_with_pii = session.sql(\"\"\"\nSELECT \n    r.doc_id,\n    r.document_text,\n    p.pii_detected\nFROM raw_data.fiscal_documents_raw r\nLEFT JOIN processed_data.pii_detection_temp p \n    ON r.doc_id = p.doc_id\nORDER BY r.doc_id\n\"\"\").collect()\n\nall_results = []\n\nfor i, doc in enumerate(docs_with_pii):\n    doc_id = doc['DOC_ID']\n    original_text = doc['DOCUMENT_TEXT']\n    pii_variant = doc['PII_DETECTED']\n\n    # Normalisation corrigée\n    if pii_variant:\n        if isinstance(pii_variant, str):\n            try:\n                raw_list = json.loads(pii_variant)\n            except Exception:\n                raw_list = []\n        else:\n            raw_list = list(pii_variant) if not isinstance(pii_variant, list) else pii_variant\n\n        pii_list = [\n            x for x in raw_list\n            if isinstance(x, dict) and 'text' in x\n        ]\n    else:\n        pii_list = []\n\n    anon_text, replacements, mappings = anonymize_with_strategy(\n        original_text,\n        pii_list,\n        anon_global\n    )\n\n    all_results.append({\n        'DOC_ID': doc_id,\n        'ORIGINAL_TEXT': original_text,\n        'ANONYMIZED_TEXT': anon_text,\n        'PII_DETECTED': pii_list,\n        'PII_COUNT': len(pii_list),\n        'STRATEGY_USED': replacements,\n        'PROCESSED_AT': datetime.utcnow()\n    })\n\n    if (i + 1) % 10 == 0:\n        print(f\"  ✓ {i+1}/{len(docs_with_pii)} documents anonymisés\")\n\nprint(f\"\\n✅ Anonymisation complétée\")\n\ndf_anon = pd.DataFrame(all_results)\nsdf_anon = session.create_dataframe(df_anon)\nsdf_anon.write.mode(\"overwrite\").save_as_table(\"PROCESSED_DATA.FISCAL_DOCUMENTS_ANON\")\n\nprint(f\"✅ {len(all_results)} documents chargés dans PROCESSED_DATA.FISCAL_DOCUMENTS_ANON\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d1a85ba-507e-4af0-9d5f-1651eb243645",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "doc_id_test = 1000\n\nrow = (\n    session.sql(f\"\"\"\n        SELECT \n            r.DOC_ID,\n            r.DOCUMENT_TEXT,\n            p.PII_DETECTED\n        FROM RAW_DATA.FISCAL_DOCUMENTS_RAW r\n        LEFT JOIN PROCESSED_DATA.PII_DETECTION_TEMP p\n            ON r.DOC_ID = p.DOC_ID\n        WHERE r.DOC_ID = {doc_id_test}\n    \"\"\")\n    .collect()[0]\n)\n\noriginal_text = row[\"DOCUMENT_TEXT\"]\npii_variant = row[\"PII_DETECTED\"]\n\nprint(\"DOC_ID:\", row[\"DOC_ID\"])\nprint(\"\\n--- TEXTE ORIGINAL (début) ---\\n\")\nprint(original_text[:500])\nprint(\"\\n--- PII_DETECTED (brut) ---\\n\")\nprint(pii_variant)\n\n# Normalisation corrigée\nif pii_variant:\n    if isinstance(pii_variant, str):\n        raw_list = json.loads(pii_variant)\n    else:\n        raw_list = list(pii_variant) if not isinstance(pii_variant, list) else pii_variant\n    pii_list = [x for x in raw_list if isinstance(x, dict) and 'text' in x]\nelse:\n    pii_list = []\n\nprint(\"\\n--- PII_LIST ---\")\nfor e in pii_list:\n    print(e[\"type\"], \"=>\", repr(e[\"text\"]), \"| trouvé dans texte ?\", e[\"text\"] in original_text)\n\n# Appliquer anonymisation\nanon_text, replacements, mappings = anonymize_with_strategy(\n    original_text,\n    pii_list,\n    anon_global\n)\n\nprint(\"\\n--- TEXTE ANONYMISÉ (début) ---\\n\")\nprint(anon_text[:500])\n\nprint(\"\\n--- REMPLACEMENTS ---\")\nfor r in replacements:\n    print(r)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ece0f92-cb60-41f9-b173-d1fd0334d747",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "TEST "
  },
  {
   "cell_type": "code",
   "id": "09aef45c-ef0c-4ade-8a4c-3f4e4cf1156e",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "import snowflake.snowpark as snp\nfrom snowflake.snowpark.context import get_active_session\nimport pandas as pd\nfrom collections import defaultdict\nimport re\n\n# Récupère la session Snowpark active dans le notebook Snowflake\nsession = get_active_session()\n\nclass SemanticAnonymizer:\n    \n    def __init__(self):\n        # mapping : (entity_type, original_value) -> pseudonym\n        # permet de garantir que la même valeur reçoit toujours le même pseudonyme\n        self.mapping = {}\n        self.counters = defaultdict(int)\n        # counters : compteur par type d'entité (PER, ORG, LOC, ...)\n\n        # 1. Suppression (masking)\n    def suppress(self, value, entity_type):\n        \"\"\"Suppression totale : remplace par un token générique\"\"\"\n        return f\"[{entity_type.upper()}_SUPPRIMÉ]\"\n\n        # 2. Perturbation d'âge (généralisation / bucketing)\n    def perturb_age(self, age_str):\n        \"\"\"Perturbation d'âge : remplace un âge précis par une tranche d'âges.\"\"\"\n        match = re.search(r'(\\d+)', age_str)\n        if match:\n            age = int(match.group(1))\n            start = (age // 10) * 10\n            end = start + 10\n            return f\"[{start}-{end} ans]\"\n        return age_str\n        # 3. Perturbation de montant (arrondi / bruit contrôlé)\n    def perturb_amount(self, amount_str):\n        \"\"\"Arrondi à ~1000€ prèS \"\"\"\n        # Extraire TOUS les chiffres (enlever espaces, virgules, €)\n        amount_clean = re.sub(r'[^\\d]', '', amount_str)\n        if amount_clean:\n            amount = int(amount_clean)\n            rounded = round(amount / 1000) * 1000\n            return f\"~{rounded:,}€\".replace(',', ' ')\n        return amount_str\n        #4 Tokenisation / pseudonymisation : remplace la valeur par un pseudonyme lisible\n\n    def tokenize(self, value, entity_type):\n        \"\"\"Pseudonymisation stable, garantit que la même valeur reçoit toujours le même pseudonyme,\n\"\"\"\n        key = (entity_type, value)\n         # Clé = couple (type, valeur) pour distinguer mêmes chaînes de types différents\n         # Si on n'a jamais vu cette valeur pour ce type, on crée un nouveau pseudo\n        if key not in self.mapping:\n            self.counters[entity_type] += 1\n            count = self.counters[entity_type]\n            \n            # Construit un pseudonyme lisible selon le type\n            if entity_type == 'PER':\n                pseudo = f'PERSONNE_{count}'\n            elif entity_type == 'ORG':\n                pseudo = f'ORG_{count}'\n            elif entity_type == 'LOC':\n                pseudo = f'LOC_{count}'\n            else:\n                pseudo = f'{entity_type}_{count}'\n         #  générique pour les autres types (SIRET, EMAIL, IBAN, etc.)\n            self.mapping[key] = pseudo\n        \n        return self.mapping[key]\n         # Retourne toujours le même pseudonyme pour ce (type, valeur)\n\n    def get_mapping_table(self):\n        \"\"\"Export du mapping pour audit\"\"\"\n        return [\n            {'pseudonym': pseudo, 'original_value': orig, 'entity_type': etype}\n            for (etype, orig), pseudo in self.mapping.items()\n        ]\n\nprint(\" Classe SemanticAnonymizer créée\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe5bd4e1-79e9-4ac6-8322-f121507d4568",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "import json\n\ndef anonymize_with_strategy(text, pii_list, anon: SemanticAnonymizer):\n    \"\"\"\n    Applique les stratégies d'anonymisation sur les PII détectées.\n    \n    - Filtre les entités chevauchantes (garde la plus longue)\n    - Tri par longueur décroissante puis position\n    - Remplacements de droite à gauche\n    \"\"\"\n    anon_text = text\n    replacements = []\n    \n    # 1. FILTRER les entités chevauchantes (garder la plus longue)\n    filtered_pii = []\n    sorted_by_length = sorted(pii_list, key=lambda x: len(x[\"text\"]), reverse=True)\n    \n    for pii in sorted_by_length:\n        # Vérifier si cette entité est contenue dans une entité déjà retenue\n        is_overlapping = False\n        for kept_pii in filtered_pii:\n            if pii[\"text\"] in kept_pii[\"text\"]:\n                is_overlapping = True\n                break\n        \n        if not is_overlapping:\n            filtered_pii.append(pii)\n    \n    # 2. TRIER par position décroissante (remplacer de droite à gauche)\n    sorted_pii = sorted(\n        filtered_pii,\n        key=lambda x: text.find(x[\"text\"]),\n        reverse=True\n    )\n    \n    # 3. TRAITER IBAN en priorité (avant les numéros de téléphone)\n    for pii in sorted_pii:\n        entity_type = pii[\"type\"]\n        original = pii[\"text\"]\n        \n        if entity_type != \"IBAN\":\n            continue\n        \n        replacement = anon.suppress(original, entity_type)\n        \n        if original in anon_text:\n            anon_text = anon_text.replace(original, replacement, 1)\n        \n        replacements.append({\n            \"original\": original,\n            \"type\": entity_type,\n            \"replacement\": replacement\n        })\n    \n    # 4. TRAITER les autres entités\n    for pii in sorted_pii:\n        entity_type = pii[\"type\"]\n        original = pii[\"text\"]\n        \n        if entity_type == \"IBAN\":\n            continue\n        \n        # Stratégie par type\n        if entity_type in [\"PER\", \"ORG\", \"LOC\"]:\n            replacement = anon.tokenize(original, entity_type)\n        elif entity_type == \"AGE\":\n            replacement = anon.perturb_age(original)\n        elif entity_type == \"MONTANT\":\n            replacement = anon.perturb_amount(original)\n        elif entity_type in [\"SIRET\", \"SIREN\", \"ADRESSE\", \"CODE_POSTAL\",\n                             \"DATE\", \"DATE_NAISSANCE\", \"EMAIL\", \"TEL\", \"NIR\"]:\n            replacement = anon.suppress(original, entity_type)\n        else:\n            replacement = anon.tokenize(original, entity_type)\n        \n        if original in anon_text:\n            anon_text = anon_text.replace(original, replacement, 1)\n        \n        replacements.append({\n            \"original\": original,\n            \"type\": entity_type,\n            \"replacement\": replacement\n        })\n    \n    return anon_text, replacements, anon.get_mapping_table()\n\nprint(\"Fonction d'anonymisation créée\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7bf0516-7aaf-4026-866f-c5d40710a736",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "from datetime import datetime\nimport pandas as pd\nimport json\n\n# RÉINITIALISER l'anonymizer pour repartir des compteurs à 0\nanon_global = SemanticAnonymizer()\n\nprint(\"Script de l'anonymisation\")\n\n# 1. Charger les documents + PII\ndocs_with_pii = session.sql(\"\"\"\nSELECT \n    r.doc_id,\n    r.document_text,\n    p.pii_detected\nFROM raw_data.fiscal_documents_raw r\nLEFT JOIN processed_data.pii_detection_temp p \n    ON r.doc_id = p.doc_id\nORDER BY r.doc_id\n\"\"\").collect()\n\nall_results = []\n\n# 2. Boucle sur chaque document\nfor i, doc in enumerate(docs_with_pii):\n    doc_id = doc['DOC_ID']\n    original_text = doc['DOCUMENT_TEXT']\n    pii_variant = doc['PII_DETECTED']\n    \n    # Normalisation robuste (gère str JSON ou liste Python)\n    if pii_variant:\n        if isinstance(pii_variant, str):\n            try:\n                raw_list = json.loads(pii_variant)\n            except Exception:\n                raw_list = []\n        else:\n            raw_list = list(pii_variant) if not isinstance(pii_variant, list) else pii_variant\n        \n        pii_list = [\n            x for x in raw_list\n            if isinstance(x, dict) and 'text' in x\n        ]\n    else:\n        pii_list = []\n    \n    # Appliquer l'anonymisation\n    anon_text, replacements, mappings = anonymize_with_strategy(\n        original_text,\n        pii_list,\n        anon_global\n    )\n    \n    all_results.append({\n        'DOC_ID': doc_id,\n        'ORIGINAL_TEXT': original_text,\n        'ANONYMIZED_TEXT': anon_text,\n        'PII_DETECTED': pii_list,\n        'PII_COUNT': len(pii_list),\n        'STRATEGY_USED': replacements,\n        'PROCESSED_AT': datetime.utcnow()\n    })\n    \n    if (i + 1) % 10 == 0:\n        print(f\"  ✓ {i+1}/{len(docs_with_pii)} documents anonymisés\")\n\nprint(f\"\\n Anonymisation complétée\")\n\n# 3. Charger avec OVERWRITE pour éviter les doublons\ndf_anon = pd.DataFrame(all_results)\nsdf_anon = session.create_dataframe(df_anon)\nsdf_anon.write.mode(\"overwrite\").save_as_table(\"PROCESSED_DATA.FISCAL_DOCUMENTS_ANON\")\n\nprint(f\" {len(all_results)} documents chargés (table écrasée, pas de doublons)\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b211817b-a87a-46a5-a24a-b7fafe6bcf13",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "doc_id_test = 1000\n\nrow = (\n    session.sql(f\"\"\"\n        SELECT \n            r.DOC_ID,\n            r.DOCUMENT_TEXT,\n            p.PII_DETECTED\n        FROM RAW_DATA.FISCAL_DOCUMENTS_RAW r\n        LEFT JOIN PROCESSED_DATA.PII_DETECTION_TEMP p\n            ON r.DOC_ID = p.DOC_ID\n        WHERE r.DOC_ID = {doc_id_test}\n    \"\"\")\n    .collect()[0]\n)\n\noriginal_text = row[\"DOCUMENT_TEXT\"]\npii_variant = row[\"PII_DETECTED\"]\n\nprint(\"DOC_ID:\", row[\"DOC_ID\"])\nprint(\"\\n--- TEXTE ORIGINAL (début) ---\\n\")\nprint(original_text[:500])\nprint(\"\\n--- PII_DETECTED ---\\n\")\nprint(pii_variant)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0c0c8a1-01f4-4526-95a5-d6f0451f7261",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "import pandas as pd\n\ndf_join = (\n    session.sql(\"\"\"\n        SELECT \n            r.DOC_ID,\n            r.DOCUMENT_TEXT,\n            a.ANONYMIZED_TEXT,\n            a.PII_COUNT\n        FROM RAW_DATA.FISCAL_DOCUMENTS_RAW r\n        JOIN PROCESSED_DATA.FISCAL_DOCUMENTS_ANON a\n            ON r.DOC_ID = a.DOC_ID\n        ORDER BY r.DOC_ID\n        LIMIT 3\n    \"\"\")\n    .to_pandas()\n)\n\nfor idx in range(len(df_join)):\n    doc_id = df_join.loc[idx, \"DOC_ID\"]\n    original_text = df_join.loc[idx, \"DOCUMENT_TEXT\"]\n    anon_text = df_join.loc[idx, \"ANONYMIZED_TEXT\"]\n    pii_count = df_join.loc[idx, \"PII_COUNT\"]\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(f\"DOC_ID : {doc_id}  |  PII détectées : {pii_count}\")\n    print(\"=\" * 80 + \"\\n\")\n    \n    print(\" AVANT (texte brut) :\\n\")\n    print(original_text[:400])\n    \n    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n    \n    print(\" APRÈS (texte anonymisé) :\\n\")\n    print(anon_text[:400])\n    \n    print(\"\\n\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5d5ff53-af3c-401c-bdd9-066fded14f29",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "import json\n\ndoc_id_test = 1000\n\nrow = (\n    session.sql(f\"\"\"\n        SELECT \n            r.DOC_ID,\n            r.DOCUMENT_TEXT,\n            p.PII_DETECTED\n        FROM RAW_DATA.FISCAL_DOCUMENTS_RAW r\n        LEFT JOIN PROCESSED_DATA.PII_DETECTION_TEMP p\n            ON r.DOC_ID = p.DOC_ID\n        WHERE r.DOC_ID = {doc_id_test}\n    \"\"\")\n    .collect()[0]\n)\n\noriginal_text = row[\"DOCUMENT_TEXT\"]\npii_variant = row[\"PII_DETECTED\"]\n\nprint(\"DOC_ID:\", row[\"DOC_ID\"])\nprint(\"\\n--- TEXTE ORIGINAL (début) ---\\n\")\nprint(original_text[:500])\n\n# Normalisation\nif pii_variant:\n    if isinstance(pii_variant, str):\n        raw_list = json.loads(pii_variant)\n    else:\n        raw_list = list(pii_variant) if not isinstance(pii_variant, list) else pii_variant\n    pii_list = [x for x in raw_list if isinstance(x, dict) and 'text' in x]\nelse:\n    pii_list = []\n\nprint(f\"\\n--- {len(pii_list)} PII détectées ---\")\nfor e in pii_list:\n    print(f\"  {e['type']:15} => {repr(e['text']):30} | dans texte: {e['text'] in original_text}\")\n\n# Créer un anonymizer temporaire pour ce test\nanon_test = SemanticAnonymizer()\nanon_text, replacements, mappings = anonymize_with_strategy(\n    original_text,\n    pii_list,\n    anon_test\n)\n\nprint(\"\\n--- TEXTE ANONYMISÉ (début) ---\\n\")\nprint(anon_text[:500])\n\nprint(f\"\\n--- {len(replacements)} REMPLACEMENTS ---\")\nfor r in replacements:\n    print(f\"  {r['type']:15} | {r['original']:25} → {r['replacement']}\")\n",
   "execution_count": null
  }
 ]
}